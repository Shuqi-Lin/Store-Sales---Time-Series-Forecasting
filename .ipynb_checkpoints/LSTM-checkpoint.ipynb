{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "transparent-pointer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,confusion_matrix,r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler,PowerTransformer,StandardScaler\n",
    "from sklearn.model_selection import KFold,train_test_split\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', 1000)  # or 1000\n",
    "pd.set_option('display.max_rows', 1000)  # or 1000\n",
    "import time\n",
    "from numpy import concatenate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "brutal-spray",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in, n_out, var_name,dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1] # number of variables\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [(var_name[j]+'(t-%d)' % (i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [(var_name[j]+'(t)') for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [(var_name[j]+'(t+%d)' % (i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    return agg\n",
    "\n",
    "# Load testing dataset(Daily data)\n",
    "def read_daily_df(features,file):\n",
    "    Lake_HydMet = pd.read_csv(file,header = 0,sep = '\\t',parse_dates = ['Date'])\n",
    "    Lake_HydMet = Lake_HydMet[features]\n",
    "    return Lake_HydMet\n",
    "\n",
    "# reframe dataset\n",
    "def reframe(values,hyperparameters,var_names):\n",
    "    reframed = series_to_supervised(values, hyperparameters['time_steps'], hyperparameters['n_out'],var_names)\n",
    "    reframed = reframed.iloc[hyperparameters['time_steps']:]\n",
    "    drop_col =[]\n",
    "    n_var = len(var_names)\n",
    "    for i in range(1,hyperparameters['time_steps']+1):\n",
    "        drop_col += [n_var*i-1]\n",
    "    reframed.drop(reframed.iloc[:,drop_col],axis=1,inplace = True)\n",
    "    return reframed\n",
    "\n",
    "\n",
    "def sparse_dataset(data_X,data_y):\n",
    "    index = []\n",
    "    y = []\n",
    "    for i in range(len(data_y)):\n",
    "        if ~np.isnan(data_y[i]):\n",
    "            index.append(i)\n",
    "            y.append(data_y[i])\n",
    "    X = np.stack(data_X[index,:,:])\n",
    "    y = np.array(y)\n",
    "    return index,X,y\n",
    "\n",
    "def fit_lstm(train_X,train_y,n_batch,nb_epoch,n_neuros,dropout,verbose,loss_function):\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_neuros,  return_sequences = True,\n",
    "              input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(n_neuros, return_sequences = True))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(n_neuros))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=loss_function, optimizer='adam')\n",
    "    # fit network\n",
    "    model.fit(train_X,train_y,epochs =nb_epoch,batch_size = n_batch,verbose = verbose)\n",
    "    return model\n",
    "\n",
    "def split_dataset(train,test,time_steps):\n",
    "    # split into input and outputs\n",
    "    train_X, train_y = train[:, :-1], train[:, -1]\n",
    "    test_X, test_y = test[:, :-1], test[:, -1]\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], time_steps+1, int(train_X.shape[1]/(time_steps+1))))\n",
    "    test_X = test_X.reshape((test_X.shape[0], time_steps+1, int(test_X.shape[1]/(time_steps+1))))\n",
    "    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "    print('number of input timesteps: {}'.format(train_X.shape[1]))\n",
    "    print('number of features: {}'.format(train_X.shape[2]))\n",
    "    return train_X, train_y,test_X, test_y\n",
    "\n",
    "def plot_comparison(df,y,yhat,yhat_ts,n_date,time_steps,index,rmse,mae,r2,nutrient,test_time,ylim):\n",
    "    # Observation time\n",
    "    time = df['Date'].iloc[n_date+time_steps:].reset_index()['Date'].iloc[index] \n",
    "    # Direct comparison of observation and prediction [yhat] (data point to data point)\n",
    "    pred = pd.DataFrame(concatenate((yhat.reshape(yhat.shape[0],1),y.reshape(y.shape[0],1)), axis=1),\n",
    "                        index = time)\n",
    "    pred.columns = ['Prediction','True value']\n",
    "    # Extract the continuous timeseries from input dataset\n",
    "    time_ts = df['Date'].iloc[n_date+time_steps:]\n",
    "    # The continuous prediction yhat_ts \n",
    "    pred_ts = pd.DataFrame(yhat_ts,index = time_ts,columns = ['Prediction'])\n",
    "    # Compute the 7d rolling mean of the timeseries predction\n",
    "    pred_ts['Prediction_7d'] = pred_ts['Prediction'].rolling(7,min_periods = 1).mean()\n",
    "    # Create a continous timeseries without winter gap\n",
    "    Date = pd.DataFrame(pd.date_range(start = time_ts.iloc[0],\n",
    "                                  end = time_ts.iloc[-1]),\n",
    "                    columns = ['Date'])\n",
    "    pred_ts_gap = Date.merge(pred_ts,how = 'left',on = 'Date')\n",
    "    f1,ax1 = plt.subplots(1,2,figsize = (18,6),gridspec_kw={'width_ratios': [2, 1]})\n",
    "    pred_ts_gap.plot(x = 'Date',y = ['Prediction','Prediction_7d'],\n",
    "                     style = {'Prediction':'b-','Prediction_7d':'k-'},\n",
    "                     ax = ax1[0])\n",
    "    pred.plot(y = 'True value',style='ro',alpha = 0.7,ms = 7,ax = ax1[0])\n",
    "    ax1[0].set_ylabel(nutrient)\n",
    "    ax1[0].set_xlim((test_time[0],test_time[1]))\n",
    "    ax1[0].set_ylim(ylim)\n",
    "    ax1[0].text(0.7, 0.9, 'RMSE:{}, MAE:{}'.format(round(rmse,2),round(mae,2)), \n",
    "            horizontalalignment='center',verticalalignment='center', \n",
    "            transform=ax1[0].transAxes,fontsize='x-large')\n",
    "    ax1[0].legend(frameon=False)\n",
    "    pred.plot(x = 'True value', y = 'Prediction',kind = 'scatter',s = 20,c = 'blue',ax = ax1[1])\n",
    "    ax1[1].plot(pred['True value'],pred['True value'],lw  =1.5,color = 'black')\n",
    "    ax1[1].text(0.5, 0.8, 'R2:{}'.format(round(r2,2)), \n",
    "                horizontalalignment='center',verticalalignment='center', \n",
    "                transform=ax1[1].transAxes,fontsize='x-large')\n",
    "    return f1,pred_ts\n",
    "\n",
    "# ensure all data is float\n",
    "def predict_lstm(df,values,var_name,nutrient,test_time,hyperparameters,ylim):\n",
    "    train_X, train_y,test_X, test_y = split_dataset(train,test,hyperparameters['time_steps'])\n",
    "    train_y=Scaler_y.fit_transform(train_y.reshape(-1, 1))\n",
    "    # fit the lstm model\n",
    "    index,X,y = sparse_dataset(train_X,train_y) # stack the timeseries input together to create a 2D training input X, and a 1D lable y\n",
    "    #y_scaled = Scaler.fit_transform(y.reshape(-1,1))\n",
    "    print('number of samples: {}'.format(len(index)))\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=101) # 5-fold cross validation\n",
    "    RMSE = []\n",
    "    MAE = []\n",
    "    R2 = []\n",
    "    # fit the lstm model \n",
    "    for train_index, test_index in kf.split(X,y):\n",
    "        #print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model = fit_lstm(X_train,y_train,hyperparameters['n_batch'],hyperparameters['nb_epoch'],\n",
    "                         hyperparameters['n_neuros'],hyperparameters['dropout'],\n",
    "                         hyperparameters['verbose'],hyperparameters['loss_function'])\n",
    "        yhat = Scaler_y.inverse_transform(model.predict(X_test,batch_size = hyperparameters['n_batch']))\n",
    "        y_test = Scaler_y.inverse_transform(y_test.reshape(-1, 1))\n",
    "        rmse = mean_squared_error(y_test, yhat,squared=False)\n",
    "        mae= mean_absolute_error(y_test, yhat)\n",
    "        r2 =  r2_score(y_test, yhat)\n",
    "        RMSE.append(rmse) \n",
    "        MAE.append(mae)\n",
    "        R2.append(r2)\n",
    "        #print('Training RMSE: %.2f' %rmse)       \n",
    "    # make a prediction\n",
    "    model = fit_lstm(X,y,hyperparameters['n_batch'],hyperparameters['nb_epoch'],\n",
    "                     hyperparameters['n_neuros'],hyperparameters['dropout'],\n",
    "                     hyperparameters['verbose'],hyperparameters['loss_function'])\n",
    "    index,X,y = sparse_dataset(test_X,test_y)#sparse_dataset(test_X,Scaler_y.inverse_transform(test_y.reshape(-1, 1))) # index is the time series\n",
    "    yhat = Scaler_y.inverse_transform(model.predict(X,batch_size = hyperparameters['n_batch']))\n",
    "    rmse = mean_squared_error(y, yhat,squared=False)\n",
    "    mae = mean_absolute_error(y,yhat)\n",
    "    r2 = r2_score(y, yhat)\n",
    "    print(\"Training dataset RMSE %.2f (+/- %.2f)\" % (np.mean(RMSE), np.std(RMSE)))\n",
    "    print(\"Training dataset MAE %.2f (+/- %.2f)\" % (np.mean(MAE), np.std(MAE)))\n",
    "    print(\"Training dataset R2 %.2f (+/- %.2f)\" % (np.mean(R2), np.std(R2)))\n",
    "    # make a prediction for the whole timeseries\n",
    "    yhat_ts = Scaler_y.inverse_transform(model.predict(test_X,batch_size = hyperparameters['n_batch']))\n",
    "    figure,pred_ts = plot_comparison(df,y,yhat,yhat_ts,n_date,hyperparameters['time_steps'],index,rmse,mae,r2,nutrient,test_time,ylim)\n",
    "    return model,rmse,mae,r2,figure,pred_ts\n",
    "\n",
    "def compare(Erken_Nut,Nut_memory,nutrient,Lake_Nut_metrics,hat):\n",
    "    compare = Erken_Nut.merge(Nut_memory,on = 'Date',how = 'left')[['Date',nutrient+'_x',nutrient+'_y']].dropna()\n",
    "    compare.columns = [['Date','ML','OB']]\n",
    "    Lake_Nut_metrics[nutrient].MAE= mean_absolute_error(compare['OB'], compare['ML'])\n",
    "    Lake_Nut_metrics[nutrient].RMSE = mean_squared_error(compare['OB'], compare['ML'],squared=False)\n",
    "    Lake_Nut_metrics[nutrient].R2 = r2_score(compare['OB'], compare['ML'])\n",
    "    # Add the time-series prediction into sample dataset for next variable modeling\n",
    "    Nut_memory.loc[Nut_memory['year'].isin(test_yr),nutrient] = hat\n",
    "    return Lake_Nut_metrics,Nut_memory\n",
    "\n",
    "def predict_ts(df,nutrient,model,hyperparameters,values):\n",
    "    # add the predictive values into dataset\n",
    "    value_X, value_y = values[:, :-1], values[:, -1]\n",
    "    value_X = value_X.reshape((value_X.shape[0], hyperparameters['time_steps']+1, int(value_X.shape[1]/(hyperparameters['time_steps']+1))))\n",
    "    y_pred = Scaler_y.inverse_transform(model.predict(value_X,batch_size = hyperparameters['n_batch']))    \n",
    "    df[nutrient].iloc[hyperparameters['time_steps']:]=y_pred[:,0]\n",
    "    df[nutrient].fillna(method = 'backfill',inplace = True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lightweight-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..\\\\clustered training data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "public-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=pd.read_csv('cluster_17.csv',parse_dates=['date'])\n",
    "earthquake_families=['BEVERAGES', 'GROCERY I', 'HOME AND KITCHEN II', 'HOME APPLIANCES', 'HOME CARE', 'PERSONAL CARE', 'PET SUPPLIES']\n",
    "## Remove the sales of the families that affected by earthquake during Apr - May 2016 \n",
    "d.drop(d[(d.family.isin(earthquake_families))&(d.date>=pd.Timestamp(2016,4,1))&(d.date<pd.Timestamp(2016,6,1))].index,inplace=True)\n",
    "d=pd.concat([d,pd.get_dummies(d['family'])],axis=1).drop('family',axis=1)\n",
    "d = d.drop(['city','state','type','cluster'],axis=1)\n",
    "d.dropna(inplace=True)\n",
    "features=[list(d.columns)[i] for i in [2]+list(range(4,len(d.columns)-1))]\n",
    "target=list(d.columns)[3]\n",
    "X=d[features]\n",
    "y=d[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "noted-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyperparameters\n",
    "hyperparameters = {'n_batch':20,'nb_epoch':100,'n_neuros':100,'dropout':0.2,'time_steps':7,\n",
    "                   'n_out':1,'verbose':0,'loss_function':'mae'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "separate-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([X,y],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "partial-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "Scaler_X = MinMaxScaler()\n",
    "Scaler_y = MinMaxScaler()#PowerTransformer(standardize=False)\n",
    "start_time = time.time()\n",
    "values = df.values\n",
    "features.append(target)\n",
    "# frame as supervised learning\n",
    "reframed = reframe(values,hyperparameters,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "active-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "reframed_scaled=pd.DataFrame(Scaler_X.fit_transform(reframed.iloc[:,:-1]),\n",
    "                             columns=reframed.columns[:-1])\n",
    "target_array=pd.Series(reframed.iloc[:,-1].values.reshape(-1, 1).reshape(-1),\n",
    "             name=reframed.columns[-1])\n",
    "reframed_scaled=pd.concat([reframed_scaled,target_array],axis=1)\n",
    "values = reframed_scaled.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model,rmse,mae,r2,figure,pred_ts = predict_lstm(Nut_memory,values,features,nutrient,test_time,hyperparameters,ylim)\n",
    "# 5-fold cross validation was used to estimate the model performance\n",
    "print('Test RMSE: %.2f' % rmse)\n",
    "print('Test MAE: %.2f' % mae)\n",
    "print('Test R2: %.2f' %r2)\n",
    "Lake_Nut_metrics[nutrient]['MAE']=mae\n",
    "Lake_Nut_metrics[nutrient]['RMSE']=rmse\n",
    "Lake_Nut_metrics[nutrient]['R2']=r2\n",
    "print('Model takes '+str(round((time.time()-start_time)/60))+' min to run')\n",
    "figure.savefig(lakename+'_LSTM_'+nutrient+'.png',dpi = 500)    \n",
    "print('\\n')\n",
    "Nut_memory = predict_ts(Nut_memory,nutrient,Nut_model,hyperparameters,values)\n",
    "print('Model takes '+str(round((time.time()-start_time)/60))+' min to run')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
